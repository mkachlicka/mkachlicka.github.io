<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Current Projects</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="slidy.css">
</head>
<body>
<div id="menu">
    <nav>
        <input type="checkbox" id="show-menu" role="button">
        <label for="show-menu" class="open"><span class="fa fa-bars"></span></label>
        <label for="show-menu" class="close"><span class="fa fa-times"></span></label>
        <ul id="topnav">  
            <li><a href="index.html">Home</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="teaching.html">Teaching</a></li>
            <li><a href="resources.html">Resources</a></li>
            <li><a href="cv.html">CV</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>
</div>

<div id="container">
    <div id="pageheader">
        <h1>Current Projects</h1>
    </div>

    <div class="section">
        <div class="pageitem">
            <h2>Speech cue weighting and salience (SpeechCues)</h2>
            <p>
                Language structure is conveyed in speech by a complex set of acoustic cues, including changes in duration, amplitude, and frequency. Individuals who are better able to detect these acoustic cues may be able to more rapidly absorb the structure of a new language. However, detecting the acoustic cues of a second language can be difficult due to differences across languages in how sound patterns convey language structure. In particular, second language speakers may have difficulty directing attention towards the most relevant cues. Individuals who are better able to direct their attention to individual acoustic dimensions may be better able to focus on the acoustic cues which provide the most reliable information in an L2. We are testing the role of attention in second language learning using electroencephalography (EEG) and whether second language learners can be trained to attend to the most informative cues, and if so, whether this experience changes how they attend to sound cues.  
                <a href="https://sites.google.com/view/audioneurolab/current-projects/speech-cues?authuser=0" target="_blank">This project</a> will lead to a better understanding of why some people struggle to learn a second language more than others.</p>
                <p><b>Supervisors:</b> Prof Adam Tierney (PI), Prof Fred Dick, and Prof Kazuya Saito.</p>

            </p>
        </div>
        <p><a href="research.html">&larr; Back to Research</a></p>

        <div class="pageitem">
            <h2>Representations of natural sound categories (EnviSounds)</h2>
            <p>
                By necessity, the similarity between sounds is defined with respect to the dimensions used in making similarity judgments. Sounds might be similar according to one dimension (e.g., pitch) but dissimilar according to another dimension (e.g., amplitude). However, it is difficult to disentangle their contributions, as various properties often concur; for example, sounds with similar pitch might also have similar amplitude. Acoustic differences can also be overwritten entirely by the sounds’ meaning or common context. Thus, it is vital to assess the degree to which different object properties determine perceived similarity. This project explores whether and, if so, to what degree acoustic vs semantic information about environmental sounds contributes to similarity judgments. Building on our previous work measuring similarity between sounds, we want to extend this framework by investigating the representations of natural sound categories and various dimensions underlying those representations. We will test whether and to what degree similarity judgements depend on the dimension judged. In the real world, many dimensions are jointly and to varying degrees driving our perception of the surrounding world. Comparing the yielded representational similarity matrices (RDMs) would allow us to discover to what degree different sound properties both uniquely and in common determine perceived similarity.</p>
            <p><b>Collaborators:</b> Prof Fred Dick and Dr Jasper van den Bosch.</p>
        </div>
        <p><a href="research.html">&larr; Back to Research</a></p>

        <div class="pageitem">
            <h2>Automated speech assessments (ACAT)</h2>
            <p>
                To better support language teachers and learners, I have been exploring how automated assessment tools can increase the frequency and accessibility of feedback during L2 learning. Timely feedback is critical in second language learning, yet it is often limited in both classroom and self-study contexts. In response to this challenge, my work has of late focused on the development of automated comprehensibility assessments. In one such study (Saito, Macmillan, Kachlicka, Kunihara, & Minematsu, 2022), our team uses a machine learning approach involving DNN-HMM models (Deep Neural Networks and Hidden Markov Models) trained on automated fluency measures (articulation rate and pause ratio), pitch and amplitude variation, and phonetic posteriorgrams to predict comprehensibility ratings. The resulting scores strongly correlate with human assessments. Currently, we are developing a more user-friendly tool that doesn’t require advanced modelling to obtain such ratings. Instead, it employs regression models trained on automated segmental, prosodic, and fluency measures and incorporates this analytical pipeline into an app. Our tool predicts learner comprehensibility with remarkable accuracy that generalizes across different task conditions. These projects highlight both the importance of timely, individualized feedback in L2 learning, which is scarce, and the potential of automated tools to provide effective, scalable support.</p>
            <p><b>Collaborators:</b> Prof Kazuya Saito (PI).</p>
        </div>
        <p><a href="research.html">&larr; Back to Research</a></p>
    </div>

<div class="section">
<div id="credits">
    <div class="col"><p>Last modified 2025/10/11</p></div>    
    <!-- This part has to be kept intact under the CC-NC Licence -->
    <div class="col"><p><a href="http://mlpdesign.net">HTML & CSS</a> by MLPdesign & MKachlicka </p></div>
    <!-- CC-NC Licence credit ends here -->

</div>
</div>
</div>
</body>
</html>
